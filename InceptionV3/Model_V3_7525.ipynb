{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "train_dir = \"DataSet_V3_7525/train\"\n",
    "val_dir = \"DataSet_V3_7525/val\"\n",
    "test_dir = \"DataSet_V3_7525/test\"\n",
    "\n",
    "# Rescale images\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3897 images belonging to 14 classes.\n",
      "Found 1305 images belonging to 14 classes.\n",
      "Found 1740 images belonging to 14 classes.\n",
      "Train dataset size: 3897\n",
      "Validation dataset size: 1305\n",
      "Test dataset size: 1740\n"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    directory=train_dir,\n",
    "    batch_size=32,\n",
    "    target_size=(299, 299),\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True  # Ensure shuffling\n",
    ")\n",
    "\n",
    "# Validation data\n",
    "val_data = val_datagen.flow_from_directory(\n",
    "    directory=val_dir,\n",
    "    batch_size=32,\n",
    "    target_size=(299, 299),\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "# Test data\n",
    "test_data = test_datagen.flow_from_directory(\n",
    "    directory=test_dir,\n",
    "    batch_size=32,\n",
    "    target_size=(299, 299),\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False  # No need to shuffle test data\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {train_data.samples}\")\n",
    "print(f\"Validation dataset size: {val_data.samples}\")\n",
    "print(f\"Test dataset size: {test_data.samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = train_data.samples // train_data.batch_size\n",
    "validation_steps = val_data.samples // val_data.batch_size\n",
    "print(steps_per_epoch);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1121s\u001b[0m 9s/step - accuracy: 0.2058 - loss: 2.5149 - val_accuracy: 0.5773 - val_loss: 1.8203\n",
      "Epoch 2/10\n",
      "\u001b[1m  1/121\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:09\u001b[0m 2s/step - accuracy: 0.7500 - loss: 1.7048"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.7500 - loss: 1.7048 - val_accuracy: 0.4400 - val_loss: 1.8525\n",
      "Epoch 3/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1085s\u001b[0m 9s/step - accuracy: 0.6395 - loss: 1.6608 - val_accuracy: 0.7484 - val_loss: 1.3097\n",
      "Epoch 4/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 31ms/step - accuracy: 0.7500 - loss: 1.3539 - val_accuracy: 0.6800 - val_loss: 1.5010\n",
      "Epoch 5/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1086s\u001b[0m 9s/step - accuracy: 0.7736 - loss: 1.2116 - val_accuracy: 0.8109 - val_loss: 1.0219\n",
      "Epoch 6/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 31ms/step - accuracy: 0.8750 - loss: 0.9423 - val_accuracy: 0.8400 - val_loss: 1.0373\n",
      "Epoch 7/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m946s\u001b[0m 8s/step - accuracy: 0.8173 - loss: 0.9682 - val_accuracy: 0.8422 - val_loss: 0.8454\n",
      "Epoch 8/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.8125 - loss: 0.8526 - val_accuracy: 0.8000 - val_loss: 0.8907\n",
      "Epoch 9/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1006s\u001b[0m 8s/step - accuracy: 0.8508 - loss: 0.7917 - val_accuracy: 0.8602 - val_loss: 0.7236\n",
      "Epoch 10/10\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.9062 - loss: 0.6024 - val_accuracy: 0.9200 - val_loss: 0.8467\n"
     ]
    }
   ],
   "source": [
    "# Steps per epoch\n",
    "steps_per_epoch = train_data.samples // train_data.batch_size\n",
    "validation_steps = val_data.samples // val_data.batch_size\n",
    "\n",
    "# Number of classes\n",
    "num_classes = train_data.num_classes\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# 1. Create a base model with tf.keras.applications\n",
    "base_model = tf.keras.applications.InceptionV3(include_top=False, input_shape=(299, 299, 3))\n",
    "base_model.trainable = False  # Freeze base model\n",
    "\n",
    "# 2. Build custom model\n",
    "inputs = tf.keras.layers.Input(shape=(299, 299, 3), name=\"input-layer\")\n",
    "x = base_model(inputs)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
    "outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\", name=\"output-layer\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# 3. Compile model\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),  # Smaller learning rate\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# 4. Train the model\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_data,\n",
    "    validation_steps=validation_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m464s\u001b[0m 8s/step - accuracy: 0.8503 - loss: 0.6634\n",
      "Test accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "# 5. Evaluate on test data\n",
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "print(f\"Test accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# 6. Save the model\n",
    "model.save(\"BirdSpecies_V3_7525.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.9.13)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the model\n",
    "model = load_model('BirdSpecies_V3_7525.h5')\n",
    "\n",
    "# Path to test data\n",
    "test_dir = r'E:\\CAPSTONE\\Bird_Species_Classification\\DataSet_V3_7525\\test'\n",
    "\n",
    "# Image data generator for preprocessing\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "# Load test data\n",
    "test_data = test_datagen.flow_from_directory(\n",
    "    directory=test_dir,\n",
    "    target_size=(299, 299),  # Adjust size as per your model's input\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',  # Use 'binary' if you have two classes\n",
    "    shuffle=False  # Do not shuffle to match predictions with true labels\n",
    ")\n",
    "\n",
    "# Get class indices\n",
    "class_indices = test_data.class_indices\n",
    "class_labels = list(class_indices.keys())\n",
    "\n",
    "# Predict on test data\n",
    "predictions = model.predict(test_data)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_true = test_data.classes\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(y_true, y_pred, target_names=class_labels)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
